{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Document Pipeline Module\n",
    "Orchestrates the complete document processing workflow:\n",
    "PDF → Extract → Chunk → Embed → Upsert → Version Management\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Optional, Any\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from core.config_manager import ConfigManager\n",
    "from core.vector_store import VectorStore\n",
    "from core.embeddings import EmbeddingGenerator\n",
    "from utils.pdf_processor import PDFProcessor\n",
    "from models.domain_config import MergedDomainConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ],
   "id": "fd66a441f8696ff9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class DocumentPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end document processing pipeline.\n",
    "\n",
    "    Features:\n",
    "    - Complete PDF → Vector DB workflow\n",
    "    - Version detection and management\n",
    "    - Semantic deduplication\n",
    "    - Progress tracking\n",
    "    - Error handling with rollback\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_manager: ConfigManager):\n",
    "        \"\"\"\n",
    "        Initialize document pipeline.\n",
    "\n",
    "        Args:\n",
    "            config_manager: ConfigManager instance\n",
    "        \"\"\"\n",
    "        self.config_manager = config_manager\n",
    "\n",
    "        # Initialize components (will be created per domain)\n",
    "        self.vector_store = None\n",
    "        self.embedder = None\n",
    "        self.pdf_processor = None\n",
    "\n",
    "        logger.info(\"DocumentPipeline initialized\")\n",
    "\n",
    "    def _initialize_components(self, domain_config: MergedDomainConfig):\n",
    "        \"\"\"\n",
    "        Initialize processing components for a domain.\n",
    "\n",
    "        Args:\n",
    "            domain_config: Merged domain configuration\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing components for domain: {domain_config.domain.name}\")\n",
    "\n",
    "        # Create vector store (shared across domains)\n",
    "        if self.vector_store is None:\n",
    "            self.vector_store = VectorStore(domain_config.vector_store)\n",
    "\n",
    "        # Create embedder (domain-specific if embeddings differ)\n",
    "        self.embedder = EmbeddingGenerator(domain_config.embeddings)\n",
    "\n",
    "        # Create PDF processor (domain-specific chunking)\n",
    "        self.pdf_processor = PDFProcessor(domain_config.chunking)\n",
    "\n",
    "        logger.info(\"✅ Components initialized\")\n",
    "\n",
    "    def check_existing_versions(\n",
    "            self,\n",
    "            collection_name: str,\n",
    "            document_id: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Check for existing versions of a document.\n",
    "\n",
    "        Args:\n",
    "            collection_name: Collection to check\n",
    "            document_id: Document ID to check\n",
    "\n",
    "        Returns:\n",
    "            List of existing version info\n",
    "        \"\"\"\n",
    "        if self.vector_store is None:\n",
    "            return []\n",
    "\n",
    "        versions = self.vector_store.get_document_versions(collection_name, document_id)\n",
    "\n",
    "        if versions:\n",
    "            logger.info(f\"Found {len(versions)} existing versions for {document_id}\")\n",
    "        else:\n",
    "            logger.info(f\"No existing versions found for {document_id}\")\n",
    "\n",
    "        return versions\n",
    "\n",
    "    def detect_duplicate(\n",
    "            self,\n",
    "            collection_name: str,\n",
    "            document_embedding: Any,\n",
    "            threshold: float = 0.90\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Detect semantically similar documents (potential duplicates).\n",
    "\n",
    "        Args:\n",
    "            collection_name: Collection to search\n",
    "            document_embedding: Document-level embedding\n",
    "            threshold: Similarity threshold for duplicate detection\n",
    "\n",
    "        Returns:\n",
    "            Info about similar document if found, else None\n",
    "        \"\"\"\n",
    "        logger.info(f\"Checking for duplicates (threshold={threshold})\")\n",
    "\n",
    "        try:\n",
    "            results = self.vector_store.search(\n",
    "                collection_name=collection_name,\n",
    "                query_embedding=document_embedding,\n",
    "                top_k=1\n",
    "            )\n",
    "\n",
    "            if results[\"ids\"][0]:  # Has results\n",
    "                similarity = 1 - results[\"distances\"][0][0]  # Convert distance to similarity\n",
    "\n",
    "                if similarity >= threshold:\n",
    "                    metadata = results[\"metadatas\"][0][0]\n",
    "                    logger.info(f\"⚠️ Potential duplicate found: {metadata['filename']} \"\n",
    "                                f\"(similarity={similarity:.2%})\")\n",
    "\n",
    "                    return {\n",
    "                        \"filename\": metadata.get(\"filename\", \"unknown\"),\n",
    "                        \"document_id\": metadata.get(\"document_id\", \"unknown\"),\n",
    "                        \"version\": metadata.get(\"document_version\", \"unknown\"),\n",
    "                        \"similarity\": similarity\n",
    "                    }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Duplicate detection failed: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def process_document(\n",
    "            self,\n",
    "            domain_name: str,\n",
    "            pdf_path: str,\n",
    "            document_id: str,\n",
    "            version: str,\n",
    "            metadata: Dict[str, Any],\n",
    "            replace_versions: Optional[List[str]] = None,\n",
    "            check_duplicates: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete document processing pipeline.\n",
    "\n",
    "        Args:\n",
    "            domain_name: Domain to process for\n",
    "            pdf_path: Path to PDF file\n",
    "            document_id: Logical document ID\n",
    "            version: Document version\n",
    "            metadata: Additional metadata (department, doc_type, etc.)\n",
    "            replace_versions: List of versions to replace\n",
    "            check_duplicates: Whether to check for semantic duplicates\n",
    "\n",
    "        Returns:\n",
    "            Dict with processing results and statistics\n",
    "        \"\"\"\n",
    "        logger.info(f\"=\" * 80)\n",
    "        logger.info(f\"Starting document processing pipeline\")\n",
    "        logger.info(f\"Domain: {domain_name}\")\n",
    "        logger.info(f\"Document ID: {document_id}\")\n",
    "        logger.info(f\"Version: {version}\")\n",
    "        logger.info(f\"PDF: {pdf_path}\")\n",
    "        logger.info(f\"=\" * 80)\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load domain configuration\n",
    "            logger.info(\"Step 1/7: Loading domain configuration...\")\n",
    "            domain_config = self.config_manager.get_domain_config(domain_name)\n",
    "            collection_name = domain_config.domain.collection_name\n",
    "\n",
    "            # Step 2: Initialize components\n",
    "            logger.info(\"Step 2/7: Initializing components...\")\n",
    "            self._initialize_components(domain_config)\n",
    "\n",
    "            # Step 3: Extract and chunk PDF\n",
    "            logger.info(\"Step 3/7: Extracting and chunking PDF...\")\n",
    "            chunks = self.pdf_processor.process_pdf(pdf_path)\n",
    "\n",
    "            if not chunks:\n",
    "                raise ValueError(\"No chunks extracted from PDF\")\n",
    "\n",
    "            logger.info(f\"✅ Extracted {len(chunks)} chunks\")\n",
    "\n",
    "            # Step 4: Generate embeddings\n",
    "            logger.info(\"Step 4/7: Generating embeddings...\")\n",
    "            embeddings = self.embedder.embed_chunks(chunks, show_progress=True)\n",
    "            logger.info(f\"✅ Generated embeddings: shape={embeddings.shape}\")\n",
    "\n",
    "            # Step 5: Check for duplicates (optional)\n",
    "            duplicate_info = None\n",
    "            if check_duplicates:\n",
    "                logger.info(\"Step 5/7: Checking for duplicates...\")\n",
    "                doc_embedding = self.embedder.compute_document_embedding(chunks)\n",
    "                duplicate_info = self.detect_duplicate(collection_name, doc_embedding)\n",
    "            else:\n",
    "                logger.info(\"Step 5/7: Skipping duplicate check (disabled)\")\n",
    "\n",
    "            # Step 6: Handle version replacements\n",
    "            if replace_versions:\n",
    "                logger.info(f\"Step 6/7: Deleting {len(replace_versions)} old versions...\")\n",
    "                for old_version in replace_versions:\n",
    "                    deleted = self.vector_store.delete_by_document_id(\n",
    "                        collection_name=collection_name,\n",
    "                        document_id=document_id,\n",
    "                        version=old_version\n",
    "                    )\n",
    "                    logger.info(f\"  Deleted version {old_version}: {deleted} chunks\")\n",
    "            else:\n",
    "                logger.info(\"Step 6/7: No versions to replace\")\n",
    "\n",
    "            # Step 7: Upsert to vector store\n",
    "            logger.info(\"Step 7/7: Upserting to vector store...\")\n",
    "\n",
    "            # Generate chunk IDs\n",
    "            chunk_ids = [f\"{document_id}-{version}-chunk{i}\" for i in range(len(chunks))]\n",
    "\n",
    "            # Extract texts\n",
    "            texts = [chunk[\"text\"] for chunk in chunks]\n",
    "\n",
    "            # Build metadata for each chunk\n",
    "            upload_date = datetime.now().isoformat()\n",
    "            filename = Path(pdf_path).name\n",
    "\n",
    "            metadatas = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_metadata = {\n",
    "                    \"document_id\": document_id,\n",
    "                    \"document_version\": version,\n",
    "                    \"filename\": filename,\n",
    "                    \"domain\": domain_name,\n",
    "                    \"upload_date\": upload_date,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"page\": chunk.get(\"page\", 0),\n",
    "                    \"chunk_length\": chunk.get(\"length\", len(chunk[\"text\"]))\n",
    "                }\n",
    "\n",
    "                # Add user-provided metadata\n",
    "                chunk_metadata.update(metadata)\n",
    "                metadatas.append(chunk_metadata)\n",
    "\n",
    "            # Upsert\n",
    "            upsert_stats = self.vector_store.upsert_documents(\n",
    "                collection_name=collection_name,\n",
    "                chunk_ids=chunk_ids,\n",
    "                embeddings=embeddings,\n",
    "                texts=texts,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "\n",
    "            # Calculate processing time\n",
    "            processing_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "            # Build result\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"document_id\": document_id,\n",
    "                \"version\": version,\n",
    "                \"filename\": filename,\n",
    "                \"domain\": domain_name,\n",
    "                \"collection\": collection_name,\n",
    "                \"chunks_processed\": len(chunks),\n",
    "                \"chunks_upserted\": upsert_stats[\"chunks_upserted\"],\n",
    "                \"total_collection_chunks\": upsert_stats[\"total_docs_in_collection\"],\n",
    "                \"versions_replaced\": replace_versions or [],\n",
    "                \"duplicate_detected\": duplicate_info,\n",
    "                \"processing_time_seconds\": processing_time,\n",
    "                \"timestamp\": upload_date\n",
    "            }\n",
    "\n",
    "            logger.info(f\"=\" * 80)\n",
    "            logger.info(f\"✅ Document processing completed successfully\")\n",
    "            logger.info(f\"   Chunks: {len(chunks)}\")\n",
    "            logger.info(f\"   Processing time: {processing_time:.2f}s\")\n",
    "            logger.info(f\"   Collection size: {upsert_stats['total_docs_in_collection']} chunks\")\n",
    "            logger.info(f\"=\" * 80)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Document processing failed: {e}\")\n",
    "\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"document_id\": document_id,\n",
    "                \"version\": version,\n",
    "                \"domain\": domain_name,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "    def delete_document(\n",
    "            self,\n",
    "            domain_name: str,\n",
    "            document_id: str,\n",
    "            version: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Delete a document (or specific version).\n",
    "\n",
    "        Args:\n",
    "            domain_name: Domain name\n",
    "            document_id: Document ID to delete\n",
    "            version: Optional specific version to delete\n",
    "\n",
    "        Returns:\n",
    "            Dict with deletion results\n",
    "        \"\"\"\n",
    "        logger.info(f\"Deleting document: {document_id} (version={version or 'all'})\")\n",
    "\n",
    "        try:\n",
    "            # Load domain config\n",
    "            domain_config = self.config_manager.get_domain_config(domain_name)\n",
    "            collection_name = domain_config.domain.collection_name\n",
    "\n",
    "            # Initialize vector store if needed\n",
    "            if self.vector_store is None:\n",
    "                self.vector_store = VectorStore(domain_config.vector_store)\n",
    "\n",
    "            # Delete\n",
    "            chunks_deleted = self.vector_store.delete_by_document_id(\n",
    "                collection_name=collection_name,\n",
    "                document_id=document_id,\n",
    "                version=version\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"success\": True,\n",
    "                \"document_id\": document_id,\n",
    "                \"version\": version or \"all\",\n",
    "                \"chunks_deleted\": chunks_deleted,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            logger.info(f\"✅ Deleted {chunks_deleted} chunks\")\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Deletion failed: {e}\")\n",
    "\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"document_id\": document_id,\n",
    "                \"version\": version,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "    def list_documents(\n",
    "            self,\n",
    "            domain_name: str,\n",
    "            filters: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List documents in a domain.\n",
    "\n",
    "        Args:\n",
    "            domain_name: Domain name\n",
    "            filters: Optional metadata filters\n",
    "\n",
    "        Returns:\n",
    "            List of document info\n",
    "        \"\"\"\n",
    "        logger.info(f\"Listing documents in {domain_name}\")\n",
    "\n",
    "        try:\n",
    "            # Load domain config\n",
    "            domain_config = self.config_manager.get_domain_config(domain_name)\n",
    "            collection_name = domain_config.domain.collection_name\n",
    "\n",
    "            # Initialize vector store if needed\n",
    "            if self.vector_store is None:\n",
    "                self.vector_store = VectorStore(domain_config.vector_store)\n",
    "\n",
    "            # List documents\n",
    "            documents = self.vector_store.list_documents(collection_name, filters)\n",
    "\n",
    "            logger.info(f\"Found {len(documents)} documents\")\n",
    "            return documents\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error listing documents: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Convenience Functions\n",
    "# ============================================\n",
    "\n",
    "def create_pipeline(config_dir: str = \"configs\") -> DocumentPipeline:\n",
    "    \"\"\"\n",
    "    Factory function to create DocumentPipeline.\n",
    "\n",
    "    Args:\n",
    "        config_dir: Path to configs directory\n",
    "\n",
    "    Returns:\n",
    "        Initialized DocumentPipeline\n",
    "    \"\"\"\n",
    "    config_manager = ConfigManager(config_dir=config_dir)\n",
    "    return DocumentPipeline(config_manager)\n",
    "\n",
    "\n",
    "def quick_upload(\n",
    "        domain_name: str,\n",
    "        pdf_path: str,\n",
    "        document_id: str,\n",
    "        version: str,\n",
    "        metadata: Dict[str, Any],\n",
    "        config_dir: str = \"configs\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Quick function to upload a document.\n",
    "\n",
    "    Args:\n",
    "        domain_name: Domain name\n",
    "        pdf_path: Path to PDF\n",
    "        document_id: Document ID\n",
    "        version: Version string\n",
    "        metadata: Additional metadata\n",
    "        config_dir: Path to configs\n",
    "\n",
    "    Returns:\n",
    "        Processing results\n",
    "    \"\"\"\n",
    "    pipeline = create_pipeline(config_dir)\n",
    "    return pipeline.process_document(\n",
    "        domain_name=domain_name,\n",
    "        pdf_path=pdf_path,\n",
    "        document_id=document_id,\n",
    "        version=version,\n",
    "        metadata=metadata\n",
    "    )\n"
   ],
   "id": "312f5d34d826e02a"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
